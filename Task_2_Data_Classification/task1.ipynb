{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Toh Kah Hie\n",
    "#### Student ID: 3936897\n",
    "\n",
    "Date: 12 September 2022\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* sklearn\n",
    "* re\n",
    "* numpy\n",
    "* collections\n",
    "* random\n",
    "* nltk\n",
    "* itertools\n",
    "\n",
    "## Introduction\n",
    "In this task, given a directory of job advertisements in the form of text files, the files are loaded and explored. After some initial exploration, those data would be stored into a Dictionary. After that, pre-processing steps would be done on only the **descriptions** of each job advertisements. After pre-processing the descriptions, a Unigram would be generated to get the vocabulary of the descriptions and including the Dictionary, they will be saved into 2 text files named ```vocab.txt``` and ```job_data.txt``` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.1 Loading data and exploration\n",
    "from sklearn.datasets import load_files\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Task 1.2.1 Tokenization\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "\n",
    "# Task 1.2.4 Remove least frequent words\n",
    "from nltk.probability import *\n",
    "\n",
    "# Task 1.2.6 Build a unigram\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Task 1.2.8\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "In this task, the data is loaded from the directory into a form of dictionary. The dictionary keys would first be explored to see what fields it has. After that, some initial explorations would be done including\n",
    "* Number of data for each field\n",
    "* Content of each dictionary field\n",
    "* Number of data for each category\n",
    "* Total number of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data files\n",
    "job_data = load_files(r'data',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1.1 Number of data under each key of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get keys of loaded files\n",
    "job_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html), the 'DESCR' attribute stores the full description of the dataset. Let's check it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(job_data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, it can be seen that there is no description for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in data : 776\n",
      "Number of elements in filenames : 776\n",
      "Number of elements in target_names : 4\n",
      "Number of elements in target : 776\n",
      "\n",
      "\n",
      "Target:  {0, 1, 2, 3}\n",
      "Target Names: ['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales']\n"
     ]
    }
   ],
   "source": [
    "# Skip the last key which is 'DESCR'\n",
    "for key in list(job_data.keys())[:-1]:\n",
    "    print(\"Number of elements in\",key,\":\",len(job_data[key]))\n",
    "print('\\n')\n",
    "print('Target: ',set(job_data['target']))\n",
    "print(\"Target Names:\",job_data['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above it can be seen that there are a total of 776 data and each is assumed to be assigned a number which corresponds to the 4 target names listed. Let's check out some of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1.2 Map target values to target names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Number 0 : data\\Accounting_Finance\\Job_00417.txt\n",
      "Category Number 1 : data\\Engineering\\Job_00169.txt\n",
      "Category Number 2 : data\\Healthcare_Nursing\\Job_00426.txt\n",
      "Category Number 3 : data\\Sales\\Job_00676.txt\n"
     ]
    }
   ],
   "source": [
    "# Initialize a counter\n",
    "counter = Counter()\n",
    "# For each target number\n",
    "for category_num in range(0,4):\n",
    "    # Get data with the specific target number\n",
    "    category_data = np.where(job_data['target'] == category_num)\n",
    "    # Update the counter\n",
    "    counter[str(category_num)] += len(category_data[0])\n",
    "    # Pick a random data under that target\n",
    "    indx = random.choice(category_data[0])\n",
    "    print(\"Category Number\",str(category_num),\":\",job_data['filenames'][indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above randomly picks data under each target (0-4). It can be that each target points to target names as follows:\n",
    "* 0 - Accounting_Finance\n",
    "* 1 - Engineering\n",
    "* 2 - Healthcare_Nursing\n",
    "* 3 - Sales\n",
    "\n",
    "To make sure that each job advertisement points to exactly one category, the total sum of data for all categories should be 776, which is the number of data present in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1.3 Number of data under each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data for target 0 : 191\n",
      "Number of data for target 1 : 231\n",
      "Number of data for target 2 : 198\n",
      "Number of data for target 3 : 156\n",
      "Sum of data for all targets: 776\n"
     ]
    }
   ],
   "source": [
    "# Print number of data under each target\n",
    "[print(\"Number of data for target\",target,\":\",str(value)) for target,value in counter.items()]\n",
    "\n",
    "# Print the sum\n",
    "print('Sum of data for all targets:',sum(counter.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of data under each target does come down to 776, which indicates that each data falls under exactly one target. From there, it also shows the number of data for each job category:\n",
    "* Accounting_Finance: 191\n",
    "* Engineering: 231\n",
    "* Healthcare_Nursing: 198\n",
    "* Sales: 156"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look into some of the contents data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1.4 Contents of each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at index: 494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Title: Clinical Operations Manager\\nWebindex: 50882000\\nDescription: Job Ref  VAC**** Job Role  Clinical Operations Manager Location  South London Salary  Up to ****k per annum 10% London Weighting Company My client is seeking a dedicated and experienced Clinical Operations Manager to manage the team members at the Centre based in South London (Brixton). Daytoday you will be overseeing all clinical areas and supervision of the nursing team, coordination of doctors/surgeons/anaesthetists, being the main point of call for clinical matters and by being a part of the supervisory team. The role will be hands on and you will be expecting to lead by example in all clinical operations and provide leadership through coaching and development. The Clinical Operations Manager will lead on the positive changes within the centre from basic requirements through to strategic planning for the years ahead. It is an exciting role enabling the successful candidate to have direct impact on operations. Experience A background within sexual health or gynaecology would be a distinct advantage but not essential. However, experience of working within a high volume environment e.g. outpatients/day surgery is essential. Experience of leading and managing a team to achieve objectives, targets and excellent levels of client satisfaction is required. If you would like to apply for this post please call Emma on **** **** or send me your CV to echadwickeclypserecruitment.co.uk'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx = random.choice(range(0,len(job_data['data'])))\n",
    "print(\"Data at index:\",indx)\n",
    "job_data['data'][indx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like within the string there are multiple strings separated by '\\n'! Let's split them into individual strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title: Senior Quant Analyst  Models Calibration',\n",
       " 'Webindex: 69931225',\n",
       " 'Company: Real',\n",
       " 'Description: An opportunity for Experienced Quantitative Analysts is now available with an international banking organization based here in the City. Working on the models calibration team you will be responsible for the development of an independent model library used by the business for the pricing of Equity and Commodities derivative contracts. You will be verifying new trades, and calibrating models to ensure a consistent valuation of trading books in the equity and/or commodities markets. We are looking for a candidate with an expert knowledge of derivative pricing models and valuation processes. You will have experience with development of trading tools and/or software interfacing. We need candidates to have a strong grasp of C++ coding and development methods. You will need to have experience in Equity and/or Commodity markets derivatives, there is no preference which product set you know however. You will have a quantitative degree and ideally some sort of financial education also. This role offers the chance to develop a career with one of theUK s largest banking institutions. You will be developing cutting edge solutions and delivering value in this mission critical department. Apply online or call Richard Smith on (Apply online only) for more details of this exciting opportunity. To find out more about Real please visit (url removed)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_info = [data.splitlines() for data in job_data['data']]\n",
    "indx = random.choice(range(0,len(job_data['data'])))\n",
    "job_info[indx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it can be seen that for each job, it contains fields like 'Title', 'Webindex','Company' and 'Description'. Let's see if every single data has those fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Title': 776, 'Webindex': 776, 'Company': 687, 'Description': 776})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_pattern = r'^(\\w+): .*'\n",
    "fieldCounter = Counter()\n",
    "for data in job_info:\n",
    "#     for field in data:\n",
    "    [fieldCounter.update(re.match(field_pattern,field).groups()) for field in data]\n",
    "fieldCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that all data contains the 'Title', 'Webindex' and 'Description' with some missing the 'Company' column. Let's convert all into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Webindex', 'Company', 'Description']\n"
     ]
    }
   ],
   "source": [
    "print(list(fieldCounter.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1.5 Save the data into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': [], 'Webindex': [], 'Company': [], 'Description': []}\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store all data\n",
    "# jobDict = dict.fromkeys(fieldCounter.keys(),[]) * Cannot be used for mutable objects e.g. list\n",
    "jobDict = {key:[] for key in fieldCounter.keys()}\n",
    "print(jobDict)\n",
    "\n",
    "# Pattern to match keys within the data\n",
    "field_pattern = r'^(\\w+): (.*)'\n",
    "\n",
    "# Function to add data into the dictionary\n",
    "def addToDict(fields):\n",
    "    # If the number of fields is less than 4 we could guarantee that the Company column is missing\n",
    "    # As Company column is the only column with missing values\n",
    "    for field in fields:\n",
    "        values = re.match(field_pattern, field).groups()\n",
    "        jobDict[values[0]].append(values[1])\n",
    "    if len(fields) < 4:\n",
    "        jobDict['Company'].append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data for Title : 776\n",
      "Number of data for Webindex : 776\n",
      "Number of data for Company : 776\n",
      "Number of data for Description : 776\n"
     ]
    }
   ],
   "source": [
    "[addToDict(data) for data in job_info]\n",
    "for key in jobDict.keys():\n",
    "    print(\"Number of data for\",key,\":\",len(jobDict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all data is set properly, let's have a look at their data types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type for Title : <class 'str'> \n",
      "Example value: Finance / Accounts Asst Bromley to ****k \n",
      "\n",
      "Data Type for Webindex : <class 'str'> \n",
      "Example value: 68997528 \n",
      "\n",
      "Data Type for Company : <class 'str'> \n",
      "Example value: First Recruitment Services \n",
      "\n",
      "Data Type for Description : <class 'str'> \n",
      "Example value: Accountant (partqualified) to **** p.a. South East London Our client, a successful manufacturing company has an immediate requirement for an Accountant for permanent role in their modern offices in South East London. The Role: Credit Control Purchase / Sales Ledger Daily collection of debts by phone, letter and email. Handling of ledger accounts Handling disputed accounts and negotiating payment terms Allocating of cash and reconciliation of accounts Adhoc administration duties within the business The Person The ideal candidate will have previous experience in a Credit Control capacity, you will possess exceptional customer service and communication skills together with IT proficiency. You will need to be a part or fully qualified Accountant to be considered for this role \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first data under each key and check the datatype\n",
    "for key in jobDict.keys():\n",
    "    print(\"Data Type for\",key,\":\",type(jobDict[key][0]),\"\\nExample value:\",jobDict[key][0],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that they are all strings! It seems appriopriate for 'Title', 'Company' and 'Description' but 'Webindex' looks more appropriate to be in integer form! Let's convert all Webindex into integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type for Title : <class 'str'> \n",
      "Example value: Finance / Accounts Asst Bromley to ****k \n",
      "\n",
      "Data Type for Webindex : <class 'int'> \n",
      "Example value: 68997528 \n",
      "\n",
      "Data Type for Company : <class 'str'> \n",
      "Example value: First Recruitment Services \n",
      "\n",
      "Data Type for Description : <class 'str'> \n",
      "Example value: Accountant (partqualified) to **** p.a. South East London Our client, a successful manufacturing company has an immediate requirement for an Accountant for permanent role in their modern offices in South East London. The Role: Credit Control Purchase / Sales Ledger Daily collection of debts by phone, letter and email. Handling of ledger accounts Handling disputed accounts and negotiating payment terms Allocating of cash and reconciliation of accounts Adhoc administration duties within the business The Person The ideal candidate will have previous experience in a Credit Control capacity, you will possess exceptional customer service and communication skills together with IT proficiency. You will need to be a part or fully qualified Accountant to be considered for this role \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Webindex into integers\n",
    "jobDict['Webindex'] = [int(webindex) for webindex in jobDict['Webindex']]\n",
    "\n",
    "# Check the data types again\n",
    "for key in jobDict.keys():\n",
    "    print(\"Data Type for\",key,\":\",type(jobDict[key][0]),\"\\nExample value:\",jobDict[key][0],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Webindex are all successfully converted into integer format!. Now that all data is in the right format, it is interesting to see if the Webindex is unique across all job advertisements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values of Webindex: 776\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique values of Webindex:\",len(set(jobDict['Webindex'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webindex is indeed unique across each job advertisement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's add the target names and filenames into the dictionary as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add file names into the dictionary\n",
    "jobDict['Filename'] = job_data['filenames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data for Title : 776\n",
      "Number of data for Webindex : 776\n",
      "Number of data for Company : 776\n",
      "Number of data for Description : 776\n",
      "Number of data for Filename : 776\n",
      "Number of data for Target : 776\n"
     ]
    }
   ],
   "source": [
    "# Map target into target names and save only the target names into the dictionary\n",
    "jobDict['Target'] = [job_data['target_names'][target] for target in job_data['target']]\n",
    "for key in jobDict.keys():\n",
    "    print(\"Number of data for\",key,\":\",len(jobDict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done with some initial exploration, let's move on to pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing steps are outlined as follows:\n",
    "1. Extract information from each job advertisement. Perform the following pre-processing steps to the **description** of each job advertisement;\n",
    "\n",
    "2. Tokenize each job advertisement description. The word tokenization must use the following regular expression, ````r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"````;\n",
    "\n",
    "3. All the words must be converted into the lower case\n",
    "\n",
    "4. Remove words with length less than 2\n",
    "\n",
    "5. Remove stopwords using the provided stop words list (stopwords_en.txt).\n",
    "\n",
    "6. Remove the word that appears only once in the document collection, based on term frequency\n",
    "\n",
    "7. Remove the top 50 most frequent words based on document frequency\n",
    "\n",
    "8. Save all job advertisement text and information in a txt file\n",
    "\n",
    "9. Build a vocabulary of the cleaned job advertisement descriptions, save it in a txt file\n",
    "\n",
    "Note that pre-processing would only be done on the description of each job advertisement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.1 Lower case & Tokenization\n",
    "Tokenization would be done through the help of the ```nltk``` library with the provided regex pattern ```r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"```. Before tokenization, all words would be converted into lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Take out the descriptions alone for pre-processing\n",
    "descriptions = jobDict['Description'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize each description\n",
    "pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "def tokenizeDesc(raw_desc):\n",
    "    # Step 3. Convert to lower case\n",
    "    desc = raw_desc.lower()\n",
    "\n",
    "    # Sentence Segmentation\n",
    "    # Transform 'Hello World. Bye World.'\n",
    "    #             to\n",
    "    # ['Hello World., Bye World.']\n",
    "    sentences = sent_tokenize(desc)\n",
    "    \n",
    "    # Step 2. Tokenize each sentence into tokens\n",
    "    # Transform ['Hello World., Bye World.']\n",
    "    #                to\n",
    "    # [['Hello','World'],['Bye','World']]\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    list_tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Flatten the list of lists into a single list\n",
    "    # Transform [['Hello','World'],['Bye','World']]\n",
    "    #                to\n",
    "    # ['Hello','World','Bye','World']\n",
    "    tokenised_desc = list(chain.from_iterable(list_tokens))\n",
    "#     print(tokenised_desc)\n",
    "    return tokenised_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's created a function that helps us track the status of the descriptions, including the vocabulary sizes as well as other information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of statuses to track\n",
    "stats = ['Vocabulary Size','Number of tokens','Lexical diversity','Average number of words in description','Number of words for longest description','Number of words for shortest description','Standard deviation for number of words']\n",
    "# Generate a dictionary based on the stats\n",
    "statusDict = {key:[] for key in stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the status of descriptions\n",
    "def status(descriptions):\n",
    "    # Get all words from the descriptions\n",
    "    all_tokens = list(chain.from_iterable(descriptions))\n",
    "    \n",
    "    # Get unique words to be the vocabulary of the corpus\n",
    "    vocab = set(all_tokens)\n",
    "    \n",
    "    # Calculate values of different information including the vocabulary size, mean and standard deviation of description length\n",
    "    desc_length = [len(description) for description in descriptions]\n",
    "    values = [len(vocab),len(all_tokens),round(len(vocab)/len(all_tokens),3),round(np.mean(desc_length),3),np.max(desc_length),np.min(desc_length),round(np.std(desc_length),3)]\n",
    "    \n",
    "    # Update the status into the status dictionary\n",
    "    for i in range(0,len(values)):\n",
    "        statusDict[stats[i]].append(values[i])\n",
    "    \n",
    "    # Print stats and show changes if changes are made\n",
    "    for key in statusDict.keys():\n",
    "        if len(statusDict[key]) > 1:\n",
    "            print('{0}: {1} -> {2}'.format(key,statusDict[key][-2],statusDict[key][-1]))\n",
    "        else:\n",
    "            print('{0}: {1}'.format(key,statusDict[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the descriptions\n",
    "descriptions = [tokenizeDesc(description) for description in descriptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization, let's check out some of the processed data as well as the status!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processing:\n",
      "Senior Sales Executive ********  Comm Our client, a leading global B2B publisher, is looking to recruit a Senior Sales Executive to join their vibrant team based in South London. As Senior Sales Executive you will be responsible for proactively selling advertsing space to clients for our clients across print and online. With confidence and enthusiasm you will have the opportunity to visit clients in the UK and abroad attending consumer and trade exhibitions. As a driven and passionate individual with the goal to succeed you will respond efficiently and professionally to advertiser/promotional requests. With strong communication skills you will liaise with other departments such as production, editorial and credit control.You will be target driven and record transactions effectively ensuring advertising proposals and booking are clear. You will cooperate with the running of the sales department including IBI's foreign agents. You will have previous B2B and online experience in multimedia publishing, ideally within weeklies. If you are ambitious and energetic and want to be part of a major media organisation then apply now. The Key Responsibilities of the Senior Sales Executive include: . Organising sales meetings, exhibitions, networking and business presentations . Key account management of clients, maintaining an after sales relationship with all clients . Conducting detailed preparation and research into the sales calls you make and the potential customer you are targeting . Maintaining a strong knowledge of the industry in which you work and any factors that may affect your business . Preparing, compiling and processing spreadsheets to report and communicate with your team . Producing a personal development plan to assist your progression within this role The Key qualities we are looking for in the Senior Sales Executive include: . Previous experience within B2B media sales and online . Experience of selling on a weekly . Previous Business to Business sales experience . Previous experience in multimedia publishing . Strong confidence to sell and negotiate at all levels . Ability to communicate at senior levels . Solid ability to learn and develop new and existing skills . Ability to work individually as well as part of a team . An ambitious individual who is willing to get involved and progress within the company . A selfmotivated individual who is well organised and client focused . Excellent knowledge of Excel, PowerPoint/Outlook . A good team player with a strong work ethic You must be eligible to work in the UK to apply for this position. This job was originally posted as www.SalesTarget.co.uk/JobSeeking/SeniorSalesExecutive_job****\n",
      "\n",
      "After processing:\n",
      "['senior', 'sales', 'executive', 'comm', 'our', 'client', 'a', 'leading', 'global', 'b', 'b', 'publisher', 'is', 'looking', 'to', 'recruit', 'a', 'senior', 'sales', 'executive', 'to', 'join', 'their', 'vibrant', 'team', 'based', 'in', 'south', 'london', 'as', 'senior', 'sales', 'executive', 'you', 'will', 'be', 'responsible', 'for', 'proactively', 'selling', 'advertsing', 'space', 'to', 'clients', 'for', 'our', 'clients', 'across', 'print', 'and', 'online', 'with', 'confidence', 'and', 'enthusiasm', 'you', 'will', 'have', 'the', 'opportunity', 'to', 'visit', 'clients', 'in', 'the', 'uk', 'and', 'abroad', 'attending', 'consumer', 'and', 'trade', 'exhibitions', 'as', 'a', 'driven', 'and', 'passionate', 'individual', 'with', 'the', 'goal', 'to', 'succeed', 'you', 'will', 'respond', 'efficiently', 'and', 'professionally', 'to', 'advertiser', 'promotional', 'requests', 'with', 'strong', 'communication', 'skills', 'you', 'will', 'liaise', 'with', 'other', 'departments', 'such', 'as', 'production', 'editorial', 'and', 'credit', 'control', 'you', 'will', 'be', 'target', 'driven', 'and', 'record', 'transactions', 'effectively', 'ensuring', 'advertising', 'proposals', 'and', 'booking', 'are', 'clear', 'you', 'will', 'cooperate', 'with', 'the', 'running', 'of', 'the', 'sales', 'department', 'including', \"ibi's\", 'foreign', 'agents', 'you', 'will', 'have', 'previous', 'b', 'b', 'and', 'online', 'experience', 'in', 'multimedia', 'publishing', 'ideally', 'within', 'weeklies', 'if', 'you', 'are', 'ambitious', 'and', 'energetic', 'and', 'want', 'to', 'be', 'part', 'of', 'a', 'major', 'media', 'organisation', 'then', 'apply', 'now', 'the', 'key', 'responsibilities', 'of', 'the', 'senior', 'sales', 'executive', 'include', 'organising', 'sales', 'meetings', 'exhibitions', 'networking', 'and', 'business', 'presentations', 'key', 'account', 'management', 'of', 'clients', 'maintaining', 'an', 'after', 'sales', 'relationship', 'with', 'all', 'clients', 'conducting', 'detailed', 'preparation', 'and', 'research', 'into', 'the', 'sales', 'calls', 'you', 'make', 'and', 'the', 'potential', 'customer', 'you', 'are', 'targeting', 'maintaining', 'a', 'strong', 'knowledge', 'of', 'the', 'industry', 'in', 'which', 'you', 'work', 'and', 'any', 'factors', 'that', 'may', 'affect', 'your', 'business', 'preparing', 'compiling', 'and', 'processing', 'spreadsheets', 'to', 'report', 'and', 'communicate', 'with', 'your', 'team', 'producing', 'a', 'personal', 'development', 'plan', 'to', 'assist', 'your', 'progression', 'within', 'this', 'role', 'the', 'key', 'qualities', 'we', 'are', 'looking', 'for', 'in', 'the', 'senior', 'sales', 'executive', 'include', 'previous', 'experience', 'within', 'b', 'b', 'media', 'sales', 'and', 'online', 'experience', 'of', 'selling', 'on', 'a', 'weekly', 'previous', 'business', 'to', 'business', 'sales', 'experience', 'previous', 'experience', 'in', 'multimedia', 'publishing', 'strong', 'confidence', 'to', 'sell', 'and', 'negotiate', 'at', 'all', 'levels', 'ability', 'to', 'communicate', 'at', 'senior', 'levels', 'solid', 'ability', 'to', 'learn', 'and', 'develop', 'new', 'and', 'existing', 'skills', 'ability', 'to', 'work', 'individually', 'as', 'well', 'as', 'part', 'of', 'a', 'team', 'an', 'ambitious', 'individual', 'who', 'is', 'willing', 'to', 'get', 'involved', 'and', 'progress', 'within', 'the', 'company', 'a', 'selfmotivated', 'individual', 'who', 'is', 'well', 'organised', 'and', 'client', 'focused', 'excellent', 'knowledge', 'of', 'excel', 'powerpoint', 'outlook', 'a', 'good', 'team', 'player', 'with', 'a', 'strong', 'work', 'ethic', 'you', 'must', 'be', 'eligible', 'to', 'work', 'in', 'the', 'uk', 'to', 'apply', 'for', 'this', 'position', 'this', 'job', 'was', 'originally', 'posted', 'as', 'www', 'salestarget', 'co', 'uk', 'jobseeking', 'seniorsalesexecutive', 'job']\n"
     ]
    }
   ],
   "source": [
    "# Randomly pick a description\n",
    "indx = random.choice(range(0,len(descriptions)))\n",
    "\n",
    "print('Before processing:')\n",
    "# Get original description\n",
    "print(jobDict['Description'][indx])\n",
    "\n",
    "print('\\nAfter processing:')\n",
    "# Get tokenized descriptions\n",
    "print(descriptions[indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9834\n",
      "Number of tokens: 186952\n",
      "Lexical diversity: 0.053\n",
      "Average number of words in description: 240.918\n",
      "Number of words for longest description: 815\n",
      "Number of words for shortest description: 13\n",
      "Standard deviation for number of words: 124.978\n"
     ]
    }
   ],
   "source": [
    "status(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that around half of the tokens in the corpus are duplicated (Number of tokens vs Vocabulary Size) and the longest description could go up to 815 words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.2 Remove words with length less than 2\n",
    "Assuming that we keep words with exact length of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with length < 2: 6039\n",
      "Number of unique words with length < 2: 26\n",
      "Unique Words with length less than 2: {'e', 's', 'm', 'z', 'w', 'a', 'o', 'g', 'f', 'c', 'u', 'b', 'y', 'l', 'r', 'p', 'q', 'v', 'x', 'i', 'n', 'k', 'd', 't', 'j', 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Get words in descriptions that has lenght less than 2\n",
    "short_words = [[token for token in description if len(token)<2] for description in descriptions]\n",
    "\n",
    "# Flatten them all into a single list\n",
    "all_short_words = list(chain.from_iterable(short_words))\n",
    "\n",
    "print(\"Number of words with length < 2:\",len(all_short_words))\n",
    "print(\"Number of unique words with length < 2:\",len(set(all_short_words)))\n",
    "print('Unique Words with length less than 2:',set(all_short_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that all words with length < 2 are the alphabetical characters. Let's remove them now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Remove words with length less than 2\n",
    "descriptions = [[token for token in description if len(token)>=2] for description in descriptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that words with length less than 2, it is expected that there would be changes in the status of descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9834 -> 9808\n",
      "Number of tokens: 186952 -> 180913\n",
      "Lexical diversity: 0.053 -> 0.054\n",
      "Average number of words in description: 240.918 -> 233.135\n",
      "Number of words for longest description: 815 -> 795\n",
      "Number of words for shortest description: 13 -> 13\n",
      "Standard deviation for number of words: 124.978 -> 121.605\n"
     ]
    }
   ],
   "source": [
    "status(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that after removing those words, the vocabulary size is decreased by 26 words and number of tokens is decreased by 6039 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.3 Removing stopwords\n",
    "Stopwords listed in the provided file named ````stopwords_en.txt```` would all be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 571\n",
      "First 10 stopwords: ['nobody', 'course', 'would', 'somewhat', 'would', 'vs', 'aside', 'sometime', 'nine', 'over']\n"
     ]
    }
   ],
   "source": [
    "# Step 5. Remove stopwords from stopwords_en.txt\n",
    "with open('stopwords_en.txt','r') as f:\n",
    "    stopwords = f.read().split()\n",
    "print(\"Number of stopwords:\",len(stopwords))\n",
    "print(\"First 10 stopwords:\",random.sample(stopwords,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above it can be seen that there are a total of 571 stopwords with words like \"and\", 'you're\" and \"to\", which would be removed from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words that is present in the stopwords\n",
    "descriptions = [[token for token in description if token not in stopwords] for description in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9808 -> 9404\n",
      "Number of tokens: 180913 -> 107161\n",
      "Lexical diversity: 0.054 -> 0.088\n",
      "Average number of words in description: 233.135 -> 138.094\n",
      "Number of words for longest description: 795 -> 487\n",
      "Number of words for shortest description: 13 -> 12\n",
      "Standard deviation for number of words: 121.605 -> 73.078\n"
     ]
    }
   ],
   "source": [
    "status(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the stats it can be seen that a total of 404 unique words are removed and 73k tokens are removed from the full set of tokens! With stop words removed, the maximum number of words in the description is reduced to 487 words, the average number of words as well as the standard deviation is greatly decreased as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.4 Remove words that appears ony once\n",
    "Remove words that appeared only once through **term frequency**. This can be done through the help of ```nltk``` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens of all descriptions as a list of tokens\n",
    "all_tokens = list(chain.from_iterable(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreqDist function by nltk provides the frequency distributions of all words in the text\n",
    "term_freq = FreqDist(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that appears only once: 4186\n",
      "10 random words within: ['conforms', 'garyitscity', 'cdp', 'mds', 'assistive', 'onetoone', 'neurodegenerative', 'rydym', 'floors', 'populate']\n"
     ]
    }
   ],
   "source": [
    "# items_appear_once = list(filter(lambda x: x[1]==1,term_freq.items()))\n",
    "# hapaxes() gets words that appears only once\n",
    "words_appear_once = set(term_freq.hapaxes())\n",
    "print(\"Number of words that appears only once:\",len(words_appear_once))\n",
    "print(\"10 random words within:\",random.sample(words_appear_once,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 4186 words that appeared only once, including words like \"worwickshire\", \"benefitsmanchester\" and others. These words would be removed from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6. Remove words that appears only once\n",
    "descriptions = [[token for token in description if token not in words_appear_once] for description in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9404 -> 5218\n",
      "Number of tokens: 107161 -> 102975\n",
      "Lexical diversity: 0.088 -> 0.051\n",
      "Average number of words in description: 138.094 -> 132.7\n",
      "Number of words for longest description: 487 -> 471\n",
      "Number of words for shortest description: 12 -> 12\n",
      "Standard deviation for number of words: 73.078 -> 70.378\n"
     ]
    }
   ],
   "source": [
    "status(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing such words, the vocabulary size, number of tokens as well as other value counts has all been reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.5 Remove top 50 most frequent words based on document frequency\n",
    "This can be through getting only the unique words in each description and with the help of ```ntlk``` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('experience', 586),\n",
       " ('role', 499),\n",
       " ('work', 453),\n",
       " ('team', 431),\n",
       " ('working', 407),\n",
       " ('skills', 366),\n",
       " ('client', 358),\n",
       " ('job', 348),\n",
       " ('company', 343),\n",
       " ('business', 342),\n",
       " ('uk', 316),\n",
       " ('excellent', 309),\n",
       " ('management', 301),\n",
       " ('based', 287),\n",
       " ('apply', 286),\n",
       " ('opportunity', 280),\n",
       " ('salary', 270),\n",
       " ('required', 269),\n",
       " ('successful', 267),\n",
       " ('support', 261),\n",
       " ('join', 252),\n",
       " ('candidate', 248),\n",
       " ('service', 242),\n",
       " ('knowledge', 241),\n",
       " ('development', 235),\n",
       " ('leading', 234),\n",
       " ('high', 224),\n",
       " ('cv', 223),\n",
       " ('www', 220),\n",
       " ('manager', 220),\n",
       " ('training', 214),\n",
       " ('sales', 211),\n",
       " ('strong', 211),\n",
       " ('provide', 209),\n",
       " ('including', 209),\n",
       " ('services', 208),\n",
       " ('ability', 201),\n",
       " ('contact', 200),\n",
       " ('position', 199),\n",
       " ('recruitment', 196),\n",
       " ('full', 194),\n",
       " ('benefits', 193),\n",
       " ('posted', 192),\n",
       " ('originally', 191),\n",
       " ('jobseeking', 191),\n",
       " ('clients', 187),\n",
       " ('include', 187),\n",
       " ('good', 187),\n",
       " ('essential', 186),\n",
       " ('information', 184)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the unique set of words for each description which cause the number of token in the merged list to be the number of documents that token appeared in\n",
    "unique_tokens = list(chain.from_iterable([set(description) for description in descriptions]))\n",
    "\n",
    "# Get FreqDist of unique tokens of descriptions\n",
    "doc_freq = FreqDist(unique_tokens)\n",
    "\n",
    "# Get 50 most frequent words\n",
    "doc_freq.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the word from the 50 most frequent words\n",
    "doc_freq_words = [item[0] for item in doc_freq.most_common(50)]\n",
    "\n",
    "# Step 7. Remove 50 most frequent words\n",
    "descriptions = [[token for token in description if token not in doc_freq_words] for description in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 5218 -> 5168\n",
      "Number of tokens: 102975 -> 81205\n",
      "Lexical diversity: 0.051 -> 0.064\n",
      "Average number of words in description: 132.7 -> 104.646\n",
      "Number of words for longest description: 471 -> 401\n",
      "Number of words for shortest description: 12 -> 7\n",
      "Standard deviation for number of words: 70.378 -> 58.446\n"
     ]
    }
   ],
   "source": [
    "status(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That would be the end of pre-processing and the final vocab size is shown above, with 5168 words. Now, it is time to save the neccessary data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "In this task, the vocabulary of descriptions would be generated through the use of unigrams and saved into a text file named ```vocab.txt```. The job advertisement dictionary would also be saved as a txt file named `job_data.txt` for Task 2 use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.6 Build a Unigram from the vocab generated\n",
    "The vocab is built and modified after every pre-processing step and now a Unigram would be generated based on the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 9. Build vocabulary\n",
    "def generateVocab(descriptions):\n",
    "    # Get all tokens of all descriptions as a list of tokens\n",
    "    all_tokens = list(chain.from_iterable(descriptions))\n",
    "    # Generate a unigram from the tokens\n",
    "    unigrams = ngrams(all_tokens,n=1)\n",
    "    freq_unigram = FreqDist(unigrams)\n",
    "\n",
    "    # Sort the keys in unigram alphabetically\n",
    "    sorted_keys = sorted(freq_unigram.keys())\n",
    "    \n",
    "    # Get the words from the unigram\n",
    "    keys = [key[0] for key in sorted_keys]\n",
    "    \n",
    "    # Print the first 10 words\n",
    "    print(keys[:10])\n",
    "    print(\"Number of tokens:\",len(keys))\n",
    "    \n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aap', 'aaron', 'aat', 'abb', 'abenefit', 'aberdeen', 'abi', 'abilities', 'abreast', 'abroad']\n",
      "Number of tokens: 5168\n"
     ]
    }
   ],
   "source": [
    "keys = generateVocab(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.7 Saving the unigram into text file\n",
    "The generated unigram would be saved into a text file ````vocab.txt```` by the format **word:index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9. Save generated vocabulary to txt file\n",
    "with open('vocab.txt','w') as f:\n",
    "    for indx in range(0,len(keys)-1):\n",
    "        # Write in form of word:index\n",
    "        f.write('{0}:{1}\\n'.format(keys[indx],indx))\n",
    "    # Write last line without '\\n'\n",
    "    f.write('{0}:{1}'.format(keys[-1],len(keys)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.8 Saving the dataset into a text file\n",
    "The pre-processed descriptions would be added into the job advertisement dictionary and all will be saved into text file named `job_data.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Webindex</th>\n",
       "      <th>Company</th>\n",
       "      <th>Description</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Target</th>\n",
       "      <th>Processed_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finance / Accounts Asst Bromley to ****k</td>\n",
       "      <td>68997528</td>\n",
       "      <td>First Recruitment Services</td>\n",
       "      <td>Accountant (partqualified) to **** p.a. South ...</td>\n",
       "      <td>data\\Accounting_Finance\\Job_00382.txt</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "      <td>accountant partqualified south east london man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fund Accountant  Hedge Fund</td>\n",
       "      <td>68063513</td>\n",
       "      <td>Austin Andrew Ltd</td>\n",
       "      <td>One of the leading Hedge Funds in London is cu...</td>\n",
       "      <td>data\\Accounting_Finance\\Job_00354.txt</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "      <td>hedge funds london recruiting fund accountant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deputy Home Manager</td>\n",
       "      <td>68700336</td>\n",
       "      <td>Caritas</td>\n",
       "      <td>An exciting opportunity has arisen to join an ...</td>\n",
       "      <td>data\\Healthcare_Nursing\\Job_00547.txt</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "      <td>exciting arisen establish provider elderly car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brokers Wanted Imediate Start</td>\n",
       "      <td>67996688</td>\n",
       "      <td>OneTwoTrade</td>\n",
       "      <td>OneTwoTrade is expanding their Sales Team and ...</td>\n",
       "      <td>data\\Accounting_Finance\\Job_00246.txt</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "      <td>expanding recruiting junior trainee brokers ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RGN Nurses (Hospitals)  Penarth</td>\n",
       "      <td>71803987</td>\n",
       "      <td>Swiis Healthcare</td>\n",
       "      <td>RGN Nurses (Hospitals) Immediate fulltime and ...</td>\n",
       "      <td>data\\Healthcare_Nursing\\Job_00543.txt</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "      <td>rgn nurses hospitals fulltime part swiis hour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  Webindex  \\\n",
       "0  Finance / Accounts Asst Bromley to ****k  68997528   \n",
       "1               Fund Accountant  Hedge Fund  68063513   \n",
       "2                       Deputy Home Manager  68700336   \n",
       "3             Brokers Wanted Imediate Start  67996688   \n",
       "4           RGN Nurses (Hospitals)  Penarth  71803987   \n",
       "\n",
       "                      Company  \\\n",
       "0  First Recruitment Services   \n",
       "1           Austin Andrew Ltd   \n",
       "2                     Caritas   \n",
       "3                 OneTwoTrade   \n",
       "4            Swiis Healthcare   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Accountant (partqualified) to **** p.a. South ...   \n",
       "1  One of the leading Hedge Funds in London is cu...   \n",
       "2  An exciting opportunity has arisen to join an ...   \n",
       "3  OneTwoTrade is expanding their Sales Team and ...   \n",
       "4  RGN Nurses (Hospitals) Immediate fulltime and ...   \n",
       "\n",
       "                                Filename              Target  \\\n",
       "0  data\\Accounting_Finance\\Job_00382.txt  Accounting_Finance   \n",
       "1  data\\Accounting_Finance\\Job_00354.txt  Accounting_Finance   \n",
       "2  data\\Healthcare_Nursing\\Job_00547.txt  Healthcare_Nursing   \n",
       "3  data\\Accounting_Finance\\Job_00246.txt  Accounting_Finance   \n",
       "4  data\\Healthcare_Nursing\\Job_00543.txt  Healthcare_Nursing   \n",
       "\n",
       "                               Processed_description  \n",
       "0  accountant partqualified south east london man...  \n",
       "1  hedge funds london recruiting fund accountant ...  \n",
       "2  exciting arisen establish provider elderly car...  \n",
       "3  expanding recruiting junior trainee brokers ci...  \n",
       "4  rgn nurses hospitals fulltime part swiis hour ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8. Save all job advertisement text and information in a txt file\n",
    "# Add processed description into the dictionary as list of strings\n",
    "jobDict['Processed_description'] = [' '.join(tokens) for tokens in descriptions]\n",
    "\n",
    "# Generate pandas datafram from the job advertisement dictionary\n",
    "job_df = pd.DataFrame.from_dict(jobDict)\n",
    "# Save job_df in form of csv into a text file\n",
    "job_df.to_csv('job_data.txt',index=False)\n",
    "\n",
    "# Check out first few data of the dataframe\n",
    "job_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this task, given a set of text files categorised under separated folder, I learned how to load all files as a single dictionary of data and perform some basic text pre-processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
